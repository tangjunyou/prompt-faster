---
stepsCompleted: [1, 2, 3, 4, 5, 6]
inputDocuments:
  - docs/analysis/brainstorming-session-2025-12-12.md
documentCounts:
  briefs: 0
  research: 0
  brainstorming: 1
  projectDocs: 0
workflowType: 'prd'
lastStep: 4
project_name: 'Prompt Faster'
user_name: '耶稣'
date: '2025-12-12'
---

# Product Requirements Document - Prompt Faster

**Author:** 耶稣
**Date:** 2025-12-12

---

## Executive Summary

### 产品愿景

**Prompt Faster** 是一个帮助用户自动化迭代优化 Prompt 的工具。用户提供执行目标（Dify 工作流 API 或通用大模型 API）、测试集和优化目标，系统通过 AI "老师模型" 自动分析规律、生成候选 Prompt、评估结果并反思迭代，最终输出能通过所有测试集的高质量 Prompt。

**核心问题：** 手工优化 Prompt 缺乏系统化方法，难以复现和迭代，尤其在复杂工作流场景下效率极低。

**解决方案：** 将优化过程算法化、自动化。用户无需编写优化算法，只需提供测试集和目标，系统自动迭代；同时用户可随时介入调整迭代方向。

**目标用户：** 
- **主要用户：** 频繁迭代 LLM 应用的开发者，尤其是使用 Dify 等低代码平台的用户
- **次要用户：** AI 应用构建者、Prompt 工程师
- **非目标用户：** 仅需一次性简单 Prompt 的用户

### What Makes This Special

1. **可插拔执行引擎** — 兼容 Dify 工作流 API 与通用大模型 API，用户可根据场景灵活选择，未来可扩展更多执行后端
2. **双任务模式** — 支持固定任务（有标准输出）和创意任务（无标准输出），覆盖不同优化场景
3. **全流程可视化 + 任意节点可暂停、编辑、引导** — 自动化优化算法精妙，但用户可在迭代各环节介入、调整、引导，实现真正的人机协作
4. **实时可视化迭代过程** — 节点图流动展示、老师模型思考过程透明可见，用户实时掌握优化进展
5. **模块化架构** — 系统和产品层面高度模块化，即插即用，便于长期维护和功能扩展
6. **融合成熟方案** — 借鉴 OPRO、Reflexion、DSPy SIMBA、PromptWizard 等学术成果的核心机制
7. **效率提升目标** — 将 Prompt 优化周期从数小时人工试错缩短到分钟级自动迭代

## Project Classification

| 维度 | 分类 |
|------|------|
| **技术类型** | 本地 Web 应用 (浏览器前端 + Rust 本地服务) |
| **技术栈** | Rust 后端 + TypeScript 前端 + SQLite |
| **领域** | AI/ML 工具 (Prompt 优化) |
| **复杂度** | Medium-High (涉及 AI 迭代算法、流式输出、多模式状态管理) |
| **项目上下文** | Greenfield - 全新项目 |
| **部署策略** | 开发期本地运行 (Windows + macOS)，最终 Docker 容器化 |

### 架构原则

- **接口优先设计** — 执行目标、评估器、老师模型均通过统一 Trait/Interface 定义，实现即插即用
- **高度模块化** — 执行目标、评估算法、老师模型均可独立替换扩展
- **低维护成本** — Rust 类型系统 + 模块边界清晰，长期可维护

---

## Success Criteria

### 用户成功 (User Success)

**核心体验目标：**
1. **优化后的 Prompt 通过所有测试** — 用户设定的测试集全部通过，达到预期效果
2. **理解优化规律** — 用户不仅获得可用的 Prompt，还能理解"为什么之前不行"
3. **全程可控、非黑盒** — 用户在整个优化过程中有掌控感，随时可介入

**"值回票价"的时刻：**
- 获得一个可直接使用的高质量 Prompt
- 获得对 Prompt 优化规律的洞察和经验
- 测试集在平台持久化保存，可复用于后续优化任务

**失败场景的成功：**
- 即使测试集未能全部通过，用户仍可学习到迭代过程中的经验
- 通过人工介入最终成功，用户获得参与感和成就感
- **前提约束：** 产品的自动化迭代框架本身必须足够优秀；失败只能归因于用户参数选择不当或老师模型能力不足

### 业务成功 (Business Success)

**3个月目标：**
- 程序大幅提升 Prompt 优化效率，验证核心算法有效性
- 实现真正**开箱即用**的体验 — 区别于市面上使用困难的 Prompt 优化工具
- 自己日常使用顺畅，验证产品价值

**12个月目标：**
- 持续稳定自用 + 低维护成本
- 开放给其他用户使用：**≥ 10 个外部用户尝试**
- 如开源：**≥ 100 GitHub Stars**
- 探索商业化路径（前提：至少 50 个活跃用户 + 3 个付费意向反馈）

**关键成功指标：**
1. **算法有效性** — 优化算法设计成功，真正解决 Prompt 优化问题
2. **开箱即用体验** — 用户无需复杂配置即可上手
3. **知识传递** — 为用户带来 Prompt 优化的经验和灵感
4. **高度模块化** — 几乎可以升级迭代任何模块（包括核心算法），维护省心
5. **用户功能模块化** — 功能不定死，用户可自由组合模块
6. **生态愿景** — 后续提供优化组件，用户自由搭配；最终用户参与插件开发，形成生态

### 技术成功 (Technical Success)

**优化成功率：**
- 目标：**≥ 90%**（在老师模型能力足够、参数合理的前提下）
- 若成功率过低，说明核心算法设计失败，是项目最大风险
- **验证方式：** 官方基准测试集（10-20 个标准优化任务），每次发布前必跑

**迭代效率：**
- 瓶颈在大模型运转速度，与用户提供的 API 速度强相关
- **系统自身延迟：** < 100ms（不含大模型调用时间）
- **可选优化：** 并行执行测试集（如两条并行跑，跑完一起总结）
- **并行质量约束：** 并行模式 vs 串行模式成功率差异 < 5%
- **前提：** 不以牺牲质量为代价；先保证串行质量，再开放并行作为可选功能

**系统稳定性：**
- **断点续跑：** 断网、断电等异常情况下，迭代数据必须完整保存
- **继续迭代：** 用户可在恢复后继续当前迭代，不丢失已消耗的时间和 Token
- **断点恢复测试场景覆盖：**
  - 进程被杀死（kill -9）
  - 网络断开中途
  - 数据库写入中途断电
  - 跨版本升级后恢复旧数据
- **设计共性：** 断点续跑与"用户介入迭代"本质相同 — 暂停、修改中间内容、继续

**可维护性：**
- **核心框架模块化：** 甚至可以替换核心算法，而不影响其他模块
- **核心算法边界定义：**
  - 规律抽取模块
  - Prompt 生成模块
  - 评估反馈模块
  - 反思迭代模块
- **用户功能模块化：** 用户可通过模块组合实现灵活的优化策略
- **开发者友好：** 天马行空的迭代不需要付出大代价
- **用户友好：** 更多参与点，后续可使用更多模块化功能
- **回归测试保障：**
  - 模块修改后回归测试通过率：100%
  - 核心流程端到端测试覆盖率：≥ 80%

**质量保障前提：**
- 必须先有成熟的 Prompt 优化算法和完备的调优体系
- 避免"高度自定义但找不到北" — 用户在官方框架内选择功能，保证基本质量
- **官方推荐配置：** 作为现阶段最优默认参数

### 可量化指标 (Measurable Outcomes)

| 指标 | 目标值 | 说明 |
|------|--------|------|
| 优化成功率 | ≥ 90% | 官方基准测试集验证 |
| 系统自身延迟 | < 100ms | 不含大模型调用时间 |
| 断点恢复率 | 100% | 覆盖 kill/断网/断电/跨版本 |
| 安装时间 | < 5 分钟 | 开箱即用验证 |
| 首次完成优化 | < 30 分钟 | 新用户体验验证 |
| 新增执行引擎 | < 4 小时 | 实现 ExecutionTarget trait |
| 新增评估器 | < 2 小时 | 实现 Evaluator trait |
| 新增老师模型 | < 2 小时 | 实现 TeacherModel trait |
| 核心算法替换 | 仅影响算法模块 | 模块化隔离验证 |
| 端到端测试覆盖率 | ≥ 80% | 核心流程覆盖 |
| 并行 vs 串行差异 | < 5% | 并行不牺牲质量 |
| 12个月外部用户 | ≥ 10 人 | 产品价值验证 |
| 12个月 GitHub Stars | ≥ 100 | 如开源 |

---

## Product Scope

### MVP - 最小可行产品

**核心功能：**
1. **执行引擎** — 支持 Dify 工作流 API + 通用大模型 API（至少一种）
2. **测试集管理** — 创建、编辑、持久化保存测试集
3. **自动迭代优化** — 核心算法：规律抽取 → Prompt 生成 → 评估反馈 → 反思迭代
4. **老师模型集成** — 至少支持一个老师模型配置
5. **断点续跑** — 迭代数据持久化，支持中断恢复
6. **用户介入** — 任意节点可暂停、查看、编辑、继续
7. **基础可视化** — 展示迭代过程和结果

**MVP 验收标准：**
- 端到端完成一次 Prompt 优化任务
- 优化后 Prompt 通过用户测试集
- 开箱即用：安装 < 5 分钟，首次优化 < 30 分钟

### Growth Features (Post-MVP)

1. **并行测试集执行** — 提升迭代效率（需配套算法适配，差异 < 5%）
2. **多老师模型支持** — 可配置多个老师模型，按场景选择
3. **更多执行引擎** — 扩展支持其他 LLM 平台
4. **高级可视化** — 节点图流动展示、思考过程透明
5. **模块化功能组合** — 用户可自由组合优化模块
6. **优化历史分析** — 跨任务的优化规律洞察

### Vision (Future)

1. **插件生态** — 用户可开发和共享优化组件/插件
2. **社区驱动** — 用户参与功能开发，形成生态
3. **商业化** — 付费高级功能、企业版（前提：50+ 活跃用户 + 3 付费意向）
4. **官方优化模板库** — 经过验证的优化配置，开箱即用

---

## User Journeys

### 用户画像

| 用户类型 | 特征 | 典型场景 |
|----------|------|----------|
| **低代码 AI 开发者** | 产品经理/运营，使用 Dify 等低代码平台，技术能力中等 | 构建业务工作流，长文本信息提取、数据结构化 |
| **专业 AI 开发者** | 有编程能力，直接调用 LLM API，追求效率提升 | 复杂 Prompt 工程，已有测试集，需要系统化优化 |

**共同痛点**：都有明确的测试集和质量标准，但手工迭代 Prompt 效率极低、缺乏系统化方法。

**非目标用户**：只想一次性美化 Prompt 的用户——他们的需求是表达细化，不是迭代优化。

---

### Journey 1: 李明 — 被长文本信息提取逼疯的产品经理

李明是一家中型企业的产品经理，负责用 Dify 搭建一个"合同要素提取"的内部工具。他的 Dify 工作流早就搭好了，逻辑也清晰——输入一份合同，输出关键要素（甲乙方、金额、期限、违约条款等）。

问题出在那个 System Prompt 上。无论他怎么调整措辞，总有几个测试合同提取不准：要么漏掉了隐藏在长段落里的金额，要么把备注当成了正文条款。他已经反复调了两周，每次改完 A 场景能过了，B 场景又挂了。他甚至建了一个 Excel 记录每次修改，但回头看时发现自己在兜圈子——有些失败方向他试过三次了。

一个深夜，李明在搜索"Dify prompt 优化"时发现了 Prompt Faster。他抱着试试看的心态安装了（不到 5 分钟就跑起来了），把他的 20 组测试合同和标准答案导入系统。

第一次运行自动优化，他看着节点图上的数据流动——规律抽取、Prompt 生成、测试反馈、反思迭代——整个过程透明可见。15 分钟后，系统给出了一个他从没想到的 Prompt 结构：把长文本信息提取拆成"定位→提取→校验"三步，并用 Markdown 表格约束输出格式。

他运行测试集：18/20 通过！系统继续迭代，他在"反思"节点看到老师模型分析了那 2 个失败案例——原来是合同里有"以下金额以大写为准"这类特殊表述。他手动介入，给老师模型补充了一条规则，继续跑。

20 分钟后，20/20 全部通过。李明不仅拿到了可用的 Prompt，还从迭代过程中学到了"为什么之前总是改 A 坏 B"——原来他一直在修改表面措辞，而不是抽象出提取规律。

**旅程揭示的能力需求**：测试集批量导入、可视化迭代流程、用户随时介入和引导、老师模型透明思考过程、迭代经验的知识传递

---

### Journey 2: 张薇 — 追求效率的 AI 应用开发者

张薇是一名 AI 应用开发者，日常工作是为公司构建各种 LLM 驱动的功能模块。她技术能力强，已经有了自己的 Prompt 优化方法论——建测试集、逐条调试、记录失败原因、迭代改进。但这个过程太耗时了，一个复杂 Prompt 往往要调一整天。

她的方法论没问题，但执行太机械：跑测试、看结果、分析原因、改 Prompt、再跑……每一步都要人工操作。她心想："这套流程这么确定性，为什么不能让 AI 帮我跑？"

张薇发现 Prompt Faster 时，第一反应是："这不就是我想要的吗？"她快速浏览了架构设计——可插拔执行引擎、模块化评估器——心想："如果设计合理，我甚至可以把它集成到我的工作流里。"

她配置了自己常用的 LLM API（不是 Dify），导入了 30 组测试用例。第一次运行她选择了"串行模式"保证质量。系统自动迭代了 8 轮，她全程没有介入——因为她信任这套算法逻辑。

最终 Prompt 通过了 28/30 的测试。她点开那 2 个失败案例的诊断报告，发现老师模型的分析比她预期的还要准确。她在界面上直接编辑了 Prompt 的一个细节，系统从当前节点继续迭代，2 轮后全部通过。

原本需要一整天的工作，40 分钟就完成了。张薇开始思考："这个工具的老师模型 Prompt 本身能不能优化？"——她发现系统支持这个，决定下次试试用 Prompt Faster 优化它自己的老师模型。

**旅程揭示的能力需求**：支持通用 LLM API、专家用户可"放手"让系统自动跑、任意节点断点续跑、诊断报告清晰专业、元优化能力

---

### Journey 3: 王浩 — 迭代中断后恢复的开发者（边缘场景）

王浩是一名自由开发者，正在用 Prompt Faster 优化一个客服对话生成的 Prompt。他的测试集有 50 组，迭代需要较长时间。

迭代进行到第 12 轮时，他的笔记本突然断电了。等他重新开机，心想："完了，前面跑的 Token 全浪费了。"

他打开 Prompt Faster，发现系统自动恢复到了断电前的状态——当前最佳 Prompt、已通过的测试用例、失败档案、累积的规律假设——全部都在。他点击"继续迭代"，系统从第 12 轮的断点无缝续跑。

20 分钟后，优化完成。王浩感叹："这才是生产级工具该有的样子。"

**旅程揭示的能力需求**：断点续跑（核心可靠性）、迭代状态持久化、优雅恢复不丢失已消耗的 Token

---

### Journey Requirements Summary

| 旅程 | 核心能力需求 |
|------|-------------|
| **李明（低代码开发者）** | 测试集批量导入、可视化迭代流程、用户介入引导、透明思考过程、知识传递 |
| **张薇（专业开发者）** | 通用 LLM API 支持、自动迭代模式、断点续跑、专业诊断报告、元优化 |
| **王浩（边缘场景）** | 断点续跑、状态持久化、优雅恢复 |

---

## Innovation & Novel Patterns

### 核心创新定位

**Prompt Faster 的创新不在于发明新算法，而在于将学术界和高级开发者社区已验证的 Prompt 优化方法，首次以"开箱即用"的产品形态呈现给普通用户。**

这是一种**可达性创新 (Accessibility Innovation)**：
- 现有工具（LangChain、DSPy、学术论文实现）要求用户具备编程能力
- Prompt Faster 将这些能力封装成可视化、可交互的产品体验
- 目标用户：产品经理、低代码开发者、AI 应用构建者——而非专业程序员

**类比参照**：Dify 之于 AI 应用开发 = Prompt Faster 之于 Prompt 优化

### 第一性原理重建

通过剥离假设、回归根本真理，Prompt Faster 的核心创新被重新定义：

**根本真理：**
- Prompt 优化是迭代过程，迭代需要反馈，反馈需要可见性
- 黑盒自动化只能产出结果，透明迭代才能传递知识

**重建后的核心创新：**
- Prompt Faster 的价值不是"自动化"，而是**"将迭代过程从黑盒变成玻璃盒"**
- 用户获得的不只是更好的 Prompt，更是"为什么更好"的理解
- 自动化是手段，透明可见才是目的

### 挑战的核心假设

| 既有假设 | Prompt Faster 的挑战 |
|----------|----------------------|
| Prompt 优化需要编程能力 | 通过可视化界面和自动化算法，非程序员也能完成系统化优化 |
| 优化过程是黑盒 | 全流程透明，用户可暂停、介入、引导 |
| 学术成果难以产品化 | 集成 OPRO、Reflexion、PromptWizard 等成熟方案，以产品形态落地 |
| 自动化工具只产出结果 | 玻璃盒迭代传递知识，用户理解"为什么更好" |

### 创新验证策略

**当前状态**：核心优化算法尚未完成设计，需要深入研究

**验证路径**：
1. **文献调研** — 系统梳理 OPRO、Reflexion、DSPy SIMBA、PromptWizard 等学术成果的核心机制
2. **竞品分析** — 分析 LangChain、DSPy 等框架的优化策略实现
3. **原型验证** — 在核心算法设计完成后，用官方基准测试集验证有效性
4. **迭代优化** — 基于实测数据持续改进算法

### 事前验尸：失败场景与预防

通过想象项目失败，倒推预防措施：

**最致命风险：核心算法无效**
- 预警信号：基准测试成功率 < 70%，用户反馈"还不如手调"
- 预防：早期原型验证 + 官方基准测试集 + ≥70% 成功率阈值
- 行动：如果算法成功率 <70%，立即暂停功能开发，专攻算法

**次级风险：目标用户不存在**
- 预警信号：目标用户对产品无感，"我用 ChatGPT 试几次就够了"
- 预防：早期用户访谈，量化"手工优化耗时"，确认痛点真实存在

**时间风险：开发周期过长**
- 预警信号：3 个月仍在研究，无可用原型
- 预防：算法研究时间盒 6 周，必须产出可测试原型

**竞品风险：DSPy/Dify 抢先推出类似功能**
- 预防：聚焦差异化（透明可见 + 知识传递），而非纯自动化

### 风险与承诺

**核心风险**：优化算法设计是项目成败的关键，若算法无效，产品价值归零

**应对策略**：
- **无退路策略** — 核心算法开发是重中之重，不设 fallback
- **集众家之长** — 借鉴学术论文和成熟框架，而非从零发明
- **持续迭代** — 算法模块化设计，支持后续持续优化和替换

### 差异化竞争优势

| 维度 | 现有工具 | Prompt Faster |
|------|----------|---------------|
| **目标用户** | 程序员、AI 研究者 | 产品经理、低代码开发者、AI 应用构建者 |
| **使用门槛** | 需要编程能力 | 开箱即用，可视化操作 |
| **产品形态** | 代码库/框架 | 独立应用 |
| **透明度** | 需阅读代码理解 | 可视化迭代过程、思考过程透明 |
| **人机协作** | 纯自动化 | 任意节点可暂停、编辑、引导 |
| **知识传递** | 黑盒产出结果 | 玻璃盒传递"为什么" |
