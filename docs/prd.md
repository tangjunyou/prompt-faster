---
stepsCompleted: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
inputDocuments:
  - docs/analysis/brainstorming-session-2025-12-12.md
  - docs/analysis/research/technical-algorithm-specification-research-2025-12-14.md
documentCounts:
  briefs: 0
  research: 1
  brainstorming: 1
  projectDocs: 0
workflowType: 'prd'
lastStep: 11
project_name: 'Prompt Faster'
user_name: '耶稣'
date: '2025-12-12'
---

# Product Requirements Document - Prompt Faster

**Author:** 耶稣
**Date:** 2025-12-12

---

## Executive Summary

### 产品愿景

**Prompt Faster** 是一个帮助用户自动化迭代优化 Prompt 的工具。用户提供执行目标（Dify 工作流 API 或通用大模型 API）、测试集和优化目标，系统通过 AI "老师模型" 自动分析规律、生成候选 Prompt、评估结果并反思迭代，最终输出能通过所有测试集的高质量 Prompt。

**核心问题：** 手工优化 Prompt 缺乏系统化方法，难以复现和迭代，尤其在复杂工作流场景下效率极低。

**解决方案：** 将优化过程算法化、自动化。用户无需编写优化算法，只需提供测试集和目标，系统自动迭代；同时用户可随时介入调整迭代方向。

**目标用户：** 
- **主要用户：** 频繁迭代 LLM 应用的开发者，尤其是使用 Dify 等低代码平台的用户
- **次要用户：** AI 应用构建者、Prompt 工程师
- **非目标用户：** 仅需一次性简单 Prompt 的用户

### What Makes This Special

1. **可插拔执行引擎** — 兼容 Dify 工作流 API 与通用大模型 API，用户可根据场景灵活选择，未来可扩展更多执行后端
2. **双任务模式** — 支持固定任务（有标准输出）和创意任务（无标准输出），覆盖不同优化场景
3. **全流程可视化 + 任意节点可暂停、编辑、引导** — 自动化优化算法精妙，但用户可在迭代各环节介入、调整、引导，实现真正的人机协作
4. **实时可视化迭代过程** — 节点图流动展示、老师模型思考过程透明可见，用户实时掌握优化进展
5. **模块化架构** — 系统和产品层面高度模块化，即插即用，便于长期维护和功能扩展
6. **融合成熟方案** — 借鉴 OPRO、Reflexion、DSPy SIMBA、PromptWizard 等学术成果的核心机制
7. **效率提升目标** — 将 Prompt 优化周期从数小时人工试错缩短到分钟级自动迭代

## Project Classification

| 维度 | 分类 |
|------|------|
| **技术类型** | 本地 Web 应用 (浏览器前端 + Rust 本地服务) |
| **技术栈** | Rust 后端 + TypeScript 前端 + SQLite |
| **领域** | AI/ML 工具 (Prompt 优化) |
| **复杂度** | Medium-High (涉及 AI 迭代算法、流式输出、多模式状态管理) |
| **项目上下文** | Greenfield - 全新项目 |
| **部署策略** | 开发期本地运行 (Windows + macOS)，最终 Docker 容器化 |

### 架构原则

- **接口优先设计** — 执行目标、评估器、老师模型均通过统一 Trait/Interface 定义，实现即插即用
- **高度模块化** — 执行目标、评估算法、老师模型均可独立替换扩展
- **低维护成本** — Rust 类型系统 + 模块边界清晰，长期可维护

---

## Success Criteria

### 用户成功 (User Success)

**核心体验目标：**
1. **优化后的 Prompt 通过所有测试** — 用户设定的测试集全部通过，达到预期效果
2. **理解优化规律** — 用户不仅获得可用的 Prompt，还能理解"为什么之前不行"
3. **全程可控、非黑盒** — 用户在整个优化过程中有掌控感，随时可介入

**"值回票价"的时刻：**
- 获得一个可直接使用的高质量 Prompt
- 获得对 Prompt 优化规律的洞察和经验
- 测试集在平台持久化保存，可复用于后续优化任务

**失败场景的成功：**
- 即使测试集未能全部通过，用户仍可学习到迭代过程中的经验
- 通过人工介入最终成功，用户获得参与感和成就感
- **前提约束：** 产品的自动化迭代框架本身必须足够优秀；失败只能归因于用户参数选择不当或老师模型能力不足

### 业务成功 (Business Success)

**3个月目标：**
- 程序大幅提升 Prompt 优化效率，验证核心算法有效性
- 实现真正**开箱即用**的体验 — 区别于市面上使用困难的 Prompt 优化工具
- 自己日常使用顺畅，验证产品价值

**12个月目标：**
- 持续稳定自用 + 低维护成本
- 开放给其他用户使用：**≥ 10 个外部用户尝试**
- 如开源：**≥ 100 GitHub Stars**
- 探索商业化路径（前提：至少 50 个活跃用户 + 3 个付费意向反馈）

**关键成功指标：**
1. **算法有效性** — 优化算法设计成功，真正解决 Prompt 优化问题
2. **开箱即用体验** — 用户无需复杂配置即可上手
3. **知识传递** — 为用户带来 Prompt 优化的经验和灵感
4. **高度模块化** — 几乎可以升级迭代任何模块（包括核心算法），维护省心
5. **用户功能模块化** — 功能不定死，用户可自由组合模块
6. **生态愿景** — 后续提供优化组件，用户自由搭配；最终用户参与插件开发，形成生态

### 技术成功 (Technical Success)

**优化成功率：**
- 目标：**≥ 90%**（在老师模型能力足够、参数合理的前提下）
- 若成功率过低，说明核心算法设计失败，是项目最大风险
- **验证方式：** 官方基准测试集（10-20 个标准优化任务），每次发布前必跑

**迭代效率：**
- 瓶颈在大模型运转速度，与用户提供的 API 速度强相关
- **系统自身延迟：** < 100ms（不含大模型调用时间）
- **可选优化：** 并行执行测试集（如两条并行跑，跑完一起总结）
- **并行质量约束：** 并行模式 vs 串行模式成功率差异 < 5%
- **前提：** 不以牺牲质量为代价；先保证串行质量，再开放并行作为可选功能

**系统稳定性：**
- **断点续跑：** 断网、断电等异常情况下，迭代数据必须完整保存
- **继续迭代：** 用户可在恢复后继续当前迭代，不丢失已消耗的时间和 Token
- **断点恢复测试场景覆盖：**
  - 进程被杀死（kill -9）
  - 网络断开中途
  - 数据库写入中途断电
  - 跨版本升级后恢复旧数据
- **设计共性：** 断点续跑与"用户介入迭代"本质相同 — 暂停、修改中间内容、继续

**可维护性：**
- **核心框架模块化：** 甚至可以替换核心算法，而不影响其他模块
- **核心算法边界定义：**
  - 规律抽取模块
  - Prompt 生成模块
  - 评估反馈模块
  - 反思迭代模块
- **用户功能模块化：** 用户可通过模块组合实现灵活的优化策略
- **开发者友好：** 天马行空的迭代不需要付出大代价
- **用户友好：** 更多参与点，后续可使用更多模块化功能
- **回归测试保障：**
  - 模块修改后回归测试通过率：100%
  - 核心流程端到端测试覆盖率：≥ 80%

**质量保障前提：**
- 必须先有成熟的 Prompt 优化算法和完备的调优体系
- 避免"高度自定义但找不到北" — 用户在官方框架内选择功能，保证基本质量
- **官方推荐配置：** 作为现阶段最优默认参数

### 可量化指标 (Measurable Outcomes)

| 指标 | 目标值 | 说明 |
|------|--------|------|
| 优化成功率 | ≥ 90% | 官方基准测试集验证 |
| 系统自身延迟 | < 100ms | 不含大模型调用时间 |
| 断点恢复率 | 100% | 覆盖 kill/断网/断电/跨版本 |
| 安装时间 | < 5 分钟 | 开箱即用验证 |
| 首次完成优化 | < 30 分钟 | 新用户体验验证 |
| 新增执行引擎 | < 4 小时 | 实现 ExecutionTarget trait |
| 新增评估器 | < 2 小时 | 实现 Evaluator trait |
| 新增老师模型 | < 2 小时 | 实现 TeacherModel trait |
| 核心算法替换 | 仅影响算法模块 | 模块化隔离验证 |
| 端到端测试覆盖率 | ≥ 80% | 核心流程覆盖 |
| 并行 vs 串行差异 | < 5% | 并行不牺牲质量 |
| 12个月外部用户 | ≥ 10 人 | 产品价值验证 |
| 12个月 GitHub Stars | ≥ 100 | 如开源 |

---

## Product Scope

### MVP - 最小可行产品

**核心功能：**
1. **执行引擎** — 支持 Dify 工作流 API + 通用大模型 API（至少一种）
2. **测试集管理** — 创建、编辑、持久化保存测试集
3. **自动迭代优化** — 核心算法：规律抽取 → Prompt 生成 → 评估反馈 → 反思迭代
4. **老师模型集成** — 至少支持一个老师模型配置
5. **断点续跑** — 迭代数据持久化，支持中断恢复
6. **用户介入** — 任意节点可暂停、查看、编辑、继续
7. **基础可视化** — 展示迭代过程和结果

**MVP 验收标准：**
- 端到端完成一次 Prompt 优化任务
- 优化后 Prompt 通过用户测试集
- 开箱即用：安装 < 5 分钟，首次优化 < 30 分钟

### Growth Features (Post-MVP)

1. **高级并行策略（Racing/多候选早期淘汰）** — 在基础并行之上，引入 Racing 机制实现多候选早期淘汰，进一步提升迭代效率
2. **多老师模型支持** — 可配置多个老师模型，按场景选择
3. **更多执行引擎** — 扩展支持其他 LLM 平台
4. **高级可视化** — 节点图流动展示、思考过程透明
5. **模块化功能组合** — 用户可自由组合优化模块
6. **优化历史分析** — 跨任务的优化规律洞察

### Vision (Future)

1. **插件生态** — 用户可开发和共享优化组件/插件
2. **社区驱动** — 用户参与功能开发，形成生态
3. **商业化** — 付费高级功能、企业版（前提：50+ 活跃用户 + 3 付费意向）
4. **官方优化模板库** — 经过验证的优化配置，开箱即用

---

## User Journeys

### 用户画像

| 用户类型 | 特征 | 典型场景 |
|----------|------|----------|
| **低代码 AI 开发者** | 产品经理/运营，使用 Dify 等低代码平台，技术能力中等 | 构建业务工作流，长文本信息提取、数据结构化 |
| **专业 AI 开发者** | 有编程能力，直接调用 LLM API，追求效率提升 | 复杂 Prompt 工程，已有测试集，需要系统化优化 |

**共同痛点**：都有明确的测试集和质量标准，但手工迭代 Prompt 效率极低、缺乏系统化方法。

**非目标用户**：只想一次性美化 Prompt 的用户——他们的需求是表达细化，不是迭代优化。

---

### Journey 1: 李明 — 被长文本信息提取逼疯的产品经理

李明是一家中型企业的产品经理，负责用 Dify 搭建一个"合同要素提取"的内部工具。他的 Dify 工作流早就搭好了，逻辑也清晰——输入一份合同，输出关键要素（甲乙方、金额、期限、违约条款等）。

问题出在那个 System Prompt 上。无论他怎么调整措辞，总有几个测试合同提取不准：要么漏掉了隐藏在长段落里的金额，要么把备注当成了正文条款。他已经反复调了两周，每次改完 A 场景能过了，B 场景又挂了。他甚至建了一个 Excel 记录每次修改，但回头看时发现自己在兜圈子——有些失败方向他试过三次了。

一个深夜，李明在搜索"Dify prompt 优化"时发现了 Prompt Faster。他抱着试试看的心态安装了（不到 5 分钟就跑起来了），把他的 20 组测试合同和标准答案导入系统。

第一次运行自动优化，他看着节点图上的数据流动——规律抽取、Prompt 生成、测试反馈、反思迭代——整个过程透明可见。15 分钟后，系统给出了一个他从没想到的 Prompt 结构：把长文本信息提取拆成"定位→提取→校验"三步，并用 Markdown 表格约束输出格式。

他运行测试集：18/20 通过！系统继续迭代，他在"反思"节点看到老师模型分析了那 2 个失败案例——原来是合同里有"以下金额以大写为准"这类特殊表述。他手动介入，给老师模型补充了一条规则，继续跑。

20 分钟后，20/20 全部通过。李明不仅拿到了可用的 Prompt，还从迭代过程中学到了"为什么之前总是改 A 坏 B"——原来他一直在修改表面措辞，而不是抽象出提取规律。

**旅程揭示的能力需求**：测试集批量导入、可视化迭代流程、用户随时介入和引导、老师模型透明思考过程、迭代经验的知识传递

---

### Journey 2: 张薇 — 追求效率的 AI 应用开发者

张薇是一名 AI 应用开发者，日常工作是为公司构建各种 LLM 驱动的功能模块。她技术能力强，已经有了自己的 Prompt 优化方法论——建测试集、逐条调试、记录失败原因、迭代改进。但这个过程太耗时了，一个复杂 Prompt 往往要调一整天。

她的方法论没问题，但执行太机械：跑测试、看结果、分析原因、改 Prompt、再跑……每一步都要人工操作。她心想："这套流程这么确定性，为什么不能让 AI 帮我跑？"

张薇发现 Prompt Faster 时，第一反应是："这不就是我想要的吗？"她快速浏览了架构设计——可插拔执行引擎、模块化评估器——心想："如果设计合理，我甚至可以把它集成到我的工作流里。"

她配置了自己常用的 LLM API（不是 Dify），导入了 30 组测试用例。第一次运行她选择了"串行模式"保证质量。系统自动迭代了 8 轮，她全程没有介入——因为她信任这套算法逻辑。

最终 Prompt 通过了 28/30 的测试。她点开那 2 个失败案例的诊断报告，发现老师模型的分析比她预期的还要准确。她在界面上直接编辑了 Prompt 的一个细节，系统从当前节点继续迭代，2 轮后全部通过。

原本需要一整天的工作，40 分钟就完成了。张薇开始思考："这个工具的老师模型 Prompt 本身能不能优化？"——她发现系统支持这个，决定下次试试用 Prompt Faster 优化它自己的老师模型。

**旅程揭示的能力需求**：支持通用 LLM API、专家用户可"放手"让系统自动跑、任意节点断点续跑、诊断报告清晰专业、元优化能力

---

### Journey 3: 王浩 — 迭代中断后恢复的开发者（边缘场景）

王浩是一名自由开发者，正在用 Prompt Faster 优化一个客服对话生成的 Prompt。他的测试集有 50 组，迭代需要较长时间。

迭代进行到第 12 轮时，他的笔记本突然断电了。等他重新开机，心想："完了，前面跑的 Token 全浪费了。"

他打开 Prompt Faster，发现系统自动恢复到了断电前的状态——当前最佳 Prompt、已通过的测试用例、失败档案、累积的规律假设——全部都在。他点击"继续迭代"，系统从第 12 轮的断点无缝续跑。

20 分钟后，优化完成。王浩感叹："这才是生产级工具该有的样子。"

**旅程揭示的能力需求**：断点续跑（核心可靠性）、迭代状态持久化、优雅恢复不丢失已消耗的 Token

---

### Journey Requirements Summary

| 旅程 | 核心能力需求 |
|------|-------------|
| **李明（低代码开发者）** | 测试集批量导入、可视化迭代流程、用户介入引导、透明思考过程、知识传递 |
| **张薇（专业开发者）** | 通用 LLM API 支持、自动迭代模式、断点续跑、专业诊断报告、元优化 |
| **王浩（边缘场景）** | 断点续跑、状态持久化、优雅恢复 |

---

## Innovation & Novel Patterns

### 核心创新定位

**Prompt Faster 的创新不在于发明新算法，而在于将学术界和高级开发者社区已验证的 Prompt 优化方法，首次以"开箱即用"的产品形态呈现给普通用户。**

这是一种**可达性创新 (Accessibility Innovation)**：
- 现有工具（LangChain、DSPy、学术论文实现）要求用户具备编程能力
- Prompt Faster 将这些能力封装成可视化、可交互的产品体验
- 目标用户：产品经理、低代码开发者、AI 应用构建者——而非专业程序员

**类比参照**：Dify 之于 AI 应用开发 = Prompt Faster 之于 Prompt 优化

### 第一性原理重建

通过剥离假设、回归根本真理，Prompt Faster 的核心创新被重新定义：

**根本真理：**
- Prompt 优化是迭代过程，迭代需要反馈，反馈需要可见性
- 黑盒自动化只能产出结果，透明迭代才能传递知识

**重建后的核心创新：**
- Prompt Faster 的价值不是"自动化"，而是**"将迭代过程从黑盒变成玻璃盒"**
- 用户获得的不只是更好的 Prompt，更是"为什么更好"的理解
- 自动化是手段，透明可见才是目的

### 挑战的核心假设

| 既有假设 | Prompt Faster 的挑战 |
|----------|----------------------|
| Prompt 优化需要编程能力 | 通过可视化界面和自动化算法，非程序员也能完成系统化优化 |
| 优化过程是黑盒 | 全流程透明，用户可暂停、介入、引导 |
| 学术成果难以产品化 | 集成 OPRO、Reflexion、PromptWizard 等成熟方案，以产品形态落地 |
| 自动化工具只产出结果 | 玻璃盒迭代传递知识，用户理解"为什么更好" |

### 创新验证策略

**当前状态**：核心优化算法技术规格已在《technical-algorithm-specification-research-2025-12-14.md》中定义完成，当前处于按该规格实现与验证阶段。

**已完成工作**：
1. **文献调研** — 已系统梳理 OPRO、Reflexion、DSPy SIMBA、PromptWizard 等学术成果的核心机制
2. **技术规格设计** — 已完成四层处理器架构、7 Trait 体系、三阶段迭代流程、完整数据结构定义
3. **算法配置体系** — 已设计 OptimizationConfig（含 9 个配置分组），支持灵活调优

**验证路径**：
1. **原型实现** — 按技术规格实现核心算法模块
2. **基准测试** — 用官方基准测试集验证有效性，目标 ≥70% 成功率
3. **迭代优化** — 基于实测数据持续改进算法

### 事前验尸：失败场景与预防

通过想象项目失败，倒推预防措施：

**最致命风险：核心算法无效**
- 预警信号：基准测试成功率 < 70%，用户反馈"还不如手调"
- 预防：早期原型验证 + 官方基准测试集 + ≥70% 成功率阈值
- 行动：如果算法成功率 <70%，立即暂停功能开发，专攻算法

**次级风险：目标用户不存在**
- 预警信号：目标用户对产品无感，"我用 ChatGPT 试几次就够了"
- 预防：早期用户访谈，量化"手工优化耗时"，确认痛点真实存在

**时间风险：开发周期过长**
- 预警信号：3 个月仍在研究，无可用原型
- 预防：算法研究时间盒 6 周，必须产出可测试原型

**竞品风险：DSPy/Dify 抢先推出类似功能**
- 预防：聚焦差异化（透明可见 + 知识传递），而非纯自动化

### 风险与承诺

**核心风险**：优化算法设计是项目成败的关键，若算法无效，产品价值归零

**应对策略**：
- **无退路策略** — 核心算法开发是重中之重，不设 fallback
- **集众家之长** — 借鉴学术论文和成熟框架，而非从零发明
- **持续迭代** — 算法模块化设计，支持后续持续优化和替换

### 差异化竞争优势

| 维度 | 现有工具 | Prompt Faster |
|------|----------|---------------|
| **目标用户** | 程序员、AI 研究者 | 产品经理、低代码开发者、AI 应用构建者 |
| **使用门槛** | 需要编程能力 | 开箱即用，可视化操作 |
| **产品形态** | 代码库/框架 | 独立应用 |
| **透明度** | 需阅读代码理解 | 可视化迭代过程、思考过程透明 |
| **人机协作** | 纯自动化 | 任意节点可暂停、编辑、引导 |
| **知识传递** | 黑盒产出结果 | 玻璃盒传递"为什么" |

---

## 7. 项目类型特定需求：本地 Web 应用

### 7.1 技术架构总览

```
┌─────────────────────────────────────────────────────────────┐
│                      用户的浏览器                            │
│  ┌───────────────────────────────────────────────────────┐  │
│  │          React + TypeScript 前端                       │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌──────────────┐   │  │
│  │  │ React Flow  │  │  Zustand    │  │  shadcn/ui   │   │  │
│  │  │ (节点图可视化)│  │ (状态管理)   │  │ (UI组件库)   │   │  │
│  │  └─────────────┘  └─────────────┘  └──────────────┘   │  │
│  └───────────────────────────────────────────────────────┘  │
│                           │                                  │
│              WebSocket (实时) + HTTP API (配置)               │
│                           │                                  │
└───────────────────────────┼─────────────────────────────────┘
                            │
┌───────────────────────────┼─────────────────────────────────┐
│                      用户的电脑 (Docker)                      │
│  ┌───────────────────────────────────────────────────────┐  │
│  │              Rust + Axum 后端服务                       │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌──────────────┐   │  │
│  │  │ 迭代引擎     │  │ 老师模型接口 │  │ 评估器接口    │   │  │
│  │  │ (核心算法)   │  │ (调用AI API) │  │ (结果评估)    │   │  │
│  │  └─────────────┘  └─────────────┘  └──────────────┘   │  │
│  │                         │                              │  │
│  │  ┌───────────────────────────────────────────────┐    │  │
│  │  │              SQLite 数据库 (本地文件)            │    │  │
│  │  │  Checkpoint | 测试集 | 配置 | 历史记录           │    │  │
│  │  └───────────────────────────────────────────────┘    │  │
│  └───────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

### 7.2 技术栈选型

| 层级 | 技术 | 版本 | 选型理由 |
|------|------|------|----------|
| **后端框架** | Axum | 0.8.x | Rust 生态最现代的 Web 框架，WebSocket 原生支持 |
| **异步运行时** | Tokio | 1.x | Rust 异步生态标准 |
| **数据库** | SQLite + SQLx | SQLx 0.8.x | 嵌入式、跨平台、JSON 支持完善、断电安全 |
| **前端框架** | React | 19.x | React Flow 依赖、TypeScript 支持最好、生态最丰富 |
| **可视化引擎** | React Flow | 12.x | 唯一成熟的节点图库，Dify 同款 |
| **状态管理** | Zustand + Jotai | 5.x / 2.x | 简单全局状态 + 原子化复杂状态 |
| **UI 组件** | shadcn/ui + Tailwind | 最新 | 高质量、可定制、现代化 |
| **实时通信** | WebSocket | Axum 内置 | 双向实时通信，用户介入必须 |
| **部署** | Docker Compose | V2 | 跨平台、开箱即用 |

#### 7.2.1 AI 服务商支持

| 阶段 | 支持的服务商 |
|------|----------------|
| **MVP** | 硅基流动、魔搭社区 |
| **后续扩展** | OpenAI、Anthropic、更多国内服务商 |

#### 7.2.2 模型参数配置原则

- **参数来源**：根据各平台 API 实际支持的参数设计，而非自行定义
- **通用参数**：温度(temperature)、最大上下文(max_tokens)、top_p 等
- **平台特有参数**：某些参数可能仅在特定平台生效，界面需明确标注
- **界面展示**：基础参数直接显示，高级参数折叠在「高级设置」中

### 7.3 前端架构

#### 7.3.1 页面结构与配置逻辑

**配置分层设计**：

| 配置层级 | 包含内容 | 作用范围 |
|----------|----------|----------|
| **全局配置** | 各平台 API Key（硅基流动、魔搭社区、Dify） | 所有任务共享 |
| **任务配置** | 执行目标 + 老师模型 + 测试集 + 优化目标 | 单个优化任务专属 |

**页面结构**：

| 页面 | 功能 | 核心内容 |
|------|------|----------|
| **全局设置** | 管理各平台 API Key | 服务商凭证列表、连接测试 |
| **任务列表** | 查看所有优化任务的状态 | 任务卡片、状态指示器、新建任务入口 |
| **任务详情** | 实时可视化迭代过程、用户介入 | React Flow 节点图、老师模型思考面板 |

**新建任务流程**：

```
新建优化任务时，用户需要配置：
1. 任务名称
2. 执行目标：
   - 类型：Dify 工作流 或 直接调用模型
   - 服务商/模型选择
   - 模型参数（温度、top_p 等，根据平台 API 支持情况显示）
3. 老师模型：
   - 服务商/模型选择
   - 模型参数
4. 测试集：选择已有 或 上传新测试集
5. 优化目标：描述期望的优化效果
```

#### 7.3.2 状态管理策略

| 状态类型 | 管理方案 | 示例 |
|----------|----------|------|
| **全局配置** | Zustand + persist | API Key、老师模型设置 |
| **工作区状态** | Jotai 原子化 | 当前工作区、迭代进度 |
| **实时数据** | WebSocket + Jotai | 流式输出、节点状态 |
| **组件局部** | useState/useReducer | 表单输入、UI 交互 |

#### 7.3.3 可视化需求

**节点图基础**：

- **节点类型**：规律假设节点、Prompt 生成节点、测试执行节点、反思节点、更新节点
- **边动画**：数据流动时边显示流动动画效果
- **节点状态**：待执行(灰)、执行中(蓝+脉冲动画)、成功(绿)、失败(红)、已暂停(黄)

**老师模型思考面板**：

- **位置**：节点图右侧固定面板
- **实时同步**：思考内容与当前执行的节点同步联动
- **环节标识**：面板顶部显示「当前环节：XXX」标签，与节点图中高亮节点对应
- **流式输出**：老师模型思考过程逐字符显示

#### 7.3.4 用户介入交互规则

**点击与暂停的区分**：

| 操作 | 行为 | 迭代状态 |
|------|------|----------|
| **点击节点** | 打开查看/编辑弹窗 | 继续运行，不停止 |
| **点击暂停按钮** | 停止当前迭代 | 停止运行 |

**可编辑条件**：

| 内容类型 | 是否可编辑 | 原因 |
|----------|----------|------|
| **当前迭代的中间产物**（规律假设、Prompt） | ✅ 可编辑 | 会影响后续迭代结果 |
| **历史迭代的产物** | ❌ 只读 | 已经不会参与后续迭代，编辑无意义 |
| **已产生的测试结果** | ❌ 只读 | 已经是事实，无法修改 |

**节点弹窗显示逻辑**：

```
可编辑节点弹窗：
- 标题：「编辑: XXX (迭代 #N)」
- 内容区域可编辑
- 提示：「❗ 此内容将参与后续迭代」
- 按钮：[关闭] [保存并应用]

只读节点弹窗：
- 标题：「查看: XXX (迭代 #N)」
- 内容区域只读
- 提示：「ℹ️ 历史记录，仅供查看」
- 按钮：[关闭]
```

### 7.4 后端架构

#### 7.4.1 算法与架构依赖（Algorithm Contract）

**权威规范声明**：本产品的核心算法与 Trait 体系完全以《technical-algorithm-specification-research-2025-12-14.md》为准。本 PRD 只总结关键职责与能力，不重复技术细节。**如 PRD 与技术规格在同一概念上存在冲突，以技术规格为准，并在后续 PRD 修订中对齐。**

**四层架构 ↔ Trait 映射**：

| PRD 四层架构 | 技术规格 Trait | 职责说明 |
|-------------|---------------|---------|
| Layer 1: Pattern Extractor | `RuleEngine` | 从测试用例提取规律、检测冲突、解决冲突、合并相似规律 |
| Layer 2: Prompt Engineer | `PromptGenerator` | 基于规律体系生成 Prompt |
| Layer 3: Quality Assessor | `Evaluator` | 评估执行结果，判断通过/失败 |
| Layer 4: Reflection Agent | `FeedbackAggregator` + `Optimizer` | 聚合多个反思结果、解决冲突建议、执行优化步骤 |
| (辅助) | `TeacherModel` | 底层 LLM 调用适配（generate/generate_structured） |
| (辅助) | `ExecutionTarget` | 执行目标（Dify 工作流 / 直连 AI 模型） |

**统一入口**：所有优化流程由 `OptimizationEngine` 封装，对外暴露 `run`/`resume` 接口。用户与 UI 通过此门面使用系统。

**核心数据结构映射**：

| PRD 概念 | 技术规格结构 | 说明 |
|---------|-------------|------|
| 优化上下文 | `OptimizationContext` | 贯穿整个迭代流程的共享状态 |
| 算法配置 | `OptimizationConfig` | 用户可调整的算法参数（9 个配置分组） |
| 测试用例 | `TestCase` | 支持 Dify 模式和直接 AI 模型模式 |
| 评估结果 | `EvaluationResult` | 包含 passed、score、confidence 等 |
| 迭代快照 | `Checkpoint` | 支持断点续跑和分支治理 |

> **详细字段定义**：见技术规格文档 Section 4.2

#### 7.4.2 模块化设计（Trait 概述）

以下为核心 Trait 的高层角色说明，**具体方法签名见技术规格 Section 4.2.7**：

```rust
// 执行引擎接口 — 对接不同的 AI 服务（Dify / 直连模型）
pub trait ExecutionTarget: Send + Sync {
    // execute: 执行 Prompt 并返回输出
    // execute_batch: 批量执行（支持并行优化）
}

// 评估器接口 — 判断输出是否符合预期
pub trait Evaluator: Send + Sync {
    // evaluate: 评估单个测试用例
    // evaluate_batch: 批量评估
}

// 老师模型接口 — 底层 LLM 调用适配
pub trait TeacherModel: Send + Sync {
    // generate: 生成文本响应
    // generate_structured: 生成结构化响应（JSON 模式）
}

// 规律引擎接口 — Layer 1 的核心实现
pub trait RuleEngine: Send + Sync {
    // extract_rules: 从测试用例提取规律
    // detect_conflicts: 检测规律冲突
    // resolve_conflict: 解决规律冲突
    // merge_similar_rules: 合并相似规律
}

// Prompt 生成器接口 — Layer 2 的核心实现
pub trait PromptGenerator: Send + Sync {
    // generate: 基于规律体系生成 Prompt
}

// 反馈聚合器接口 — Layer 4 聚合部分
pub trait FeedbackAggregator: Send + Sync {
    // aggregate: 聚合多个反思结果
    // arbitrate: 仲裁冲突的建议
}

// 优化器接口 — Layer 4 优化部分
pub trait Optimizer: Send + Sync {
    // optimize_step: 基于统一反馈执行一步优化
    // should_terminate: 判断是否应该终止迭代
}
```

**持久化策略**：技术规格采用 `StateManager` + `ModuleRegistry` + `Checkpoint` 组合，而非通用 `Repository` 模式。具体实现见技术规格 Section 4.3。

#### 7.4.3 扩展性保障

| 扩展场景 | 实现方式 | 预估工作量 |
|----------|----------|------------|
| 新增执行引擎（如 OpenAI API） | 实现 `ExecutionTarget` trait | < 4 小时 |
| 新增评估器（如自定义评分） | 实现 `Evaluator` trait | < 2 小时 |
| 新增老师模型适配（如 Claude） | 实现 `TeacherModel` trait | < 2 小时 |
| 新增规律引擎策略 | 实现 `RuleEngine` trait | < 8 小时 |
| 新增优化策略 | 实现 `Optimizer` trait | < 4 小时 |

### 7.5 数据持久化

#### 7.5.1 数据库配置

```rust
let opts = SqliteConnectOptions::from_str("sqlite://data.db")?
    .journal_mode(SqliteJournalMode::Wal)      // 支持并发读写
    .synchronous(SqliteSynchronous::Full)       // 断电安全
    .busy_timeout(Duration::from_secs(30))      // 写入等待超时
    .create_if_missing(true);
```

#### 7.5.2 核心数据表

| 表名 | 用途 | 关键字段 |
|------|------|----------|
| `workspaces` | 工作区管理 | id, name, config, created_at |
| `checkpoints` | 迭代快照 | id, workspace_id, iteration, step, rules(JSON), test_results(JSON), parent_id |
| `test_sets` | 测试集 | id, workspace_id, name, cases(JSON) |
| `api_credentials` | API 凭证 | id, provider, encrypted_key |
| `teacher_prompts` | 老师模型 Prompt 版本 | id, version, content, success_rate |

#### 7.5.3 Checkpoint 机制

- **存储粒度**：每个迭代步骤（rules → prompt → test → reflect → update）都存一个 Checkpoint
- **版本链**：通过 `parent_id` 形成树状结构，支持任意回滚
- **断点续跑**：启动时检查未完成的 Checkpoint，自动恢复
- **分支治理**：Checkpoint 除了 `parent_id`，还记录 `lineage_type`（Automatic/ManualPromptEdit/ManualRuleEdit/DialogueGuided/Restored），用于区分自动优化分支与人工介入分支。人工分支对元优化统计的影响见技术规格 Section 4.3。

#### 7.5.4 数据划分与防过拟合

**产品层面承诺**：通过数据划分（Train/Validation/Holdout 三分法）与防过拟合策略，保证测试集通过率具有一定泛化能力，避免"只针对已知测试集优化"的过拟合问题。

- **Train**：用于规律提炼和 Prompt 生成
- **Validation**：用于迭代过程中的评估
- **Holdout**：用于最终验证，防止过拟合

> **详细配置**：见技术规格 DataSplitConfig（Section 4.2.6.1）

### 7.6 实时通信

#### 7.6.1 通信方案

| 场景 | 协议 | 方向 |
|------|------|------|
| 流式输出老师模型思考 | WebSocket | 服务器 → 客户端 |
| 迭代进度更新 | WebSocket | 服务器 → 客户端 |
| 用户暂停/继续 | WebSocket | 客户端 → 服务器 |
| 配置/测试集 CRUD | HTTP API | 双向 |

#### 7.6.2 WebSocket 消息类型

```typescript
// 服务器 → 客户端
type ServerMessage =
  | { type: 'stream'; content: string }           // 流式输出
  | { type: 'progress'; iteration: number; step: string }  // 进度更新
  | { type: 'node_status'; nodeId: string; status: 'running' | 'success' | 'failed' }
  | { type: 'checkpoint'; data: Checkpoint }      // 新 Checkpoint

// 客户端 → 服务器
type ClientMessage =
  | { type: 'pause' }                             // 暂停
  | { type: 'resume' }                            // 继续
  | { type: 'intervene'; nodeId: string; data: any }  // 用户介入修改
```

### 7.7 部署与分发

#### 7.7.1 Docker Compose 配置

```yaml
version: '3.8'
services:
  prompt-faster:
    build:
      context: .
      platforms:
        - linux/amd64
        - linux/arm64
    ports:
      - "3000:3000"
    volumes:
      - ./data:/app/data  # 持久化数据库
    environment:
      - RUST_LOG=info
```

#### 7.7.2 跨平台支持

| 平台 | 安装方式 | 访问方式 |
|------|----------|----------|
| **Windows** | Docker Desktop + docker-compose up | http://localhost:3000 |
| **macOS (Intel)** | Docker Desktop + docker-compose up | http://localhost:3000 |
| **macOS (Apple Silicon)** | Docker Desktop + docker-compose up | http://localhost:3000 |

#### 7.7.3 未来演进路径

| 阶段 | 分发方式 |
|------|----------|
| **MVP** | Docker Compose（当前） |
| **成熟期** | 独立桌面应用（Tauri 打包） |
| **商业化** | 云托管版本（可选） |

---

## 8. Project Scoping & Phased Development

### 8.1 MVP Strategy & Philosophy

**MVP 类型**：问题解决型 MVP

**核心验证目标**：证明"Prompt 自动优化算法真的有效"

**MVP 原则**：
- **算法优先** — 核心优化算法（四层老师模型架构）必须完整实现，出品即精品，不接受简化
- **双执行引擎** — 同时支持 Dify 工作流 API 和通用大模型 API（硅基流动、魔搭社区）
- **完整人机协作** — 直接编辑 + 对话引导双模式介入
- **透明可视化** — 完整版可视化体验（节点图动画 + 流式输出 + 思考面板）

**资源需求**：单人开发，3 个月目标

### 8.2 MVP Feature Set (Phase 1)

#### 核心用户旅程支持

| 旅程 | MVP 支持程度 |
|------|-------------|
| **李明（低代码开发者）** | ✅ 完整支持 — Dify API + 可视化 + 用户介入 |
| **张薇（专业开发者）** | ✅ 完整支持 — 通用 LLM API + 自动迭代 + 断点续跑 + 元优化 |
| **王浩（边缘场景）** | ✅ 完整支持 — 断点续跑 + 状态持久化 |

#### Must-Have 功能清单

**1. 执行引擎**
- Dify 工作流 API 集成（自动解析输入变量）
- 通用大模型 API 集成（硅基流动、魔搭社区）
- 统一的 ExecutionTarget trait 设计

**2. 核心优化算法（四层架构，完整实现）**
- Layer 1: 规律抽取层 (Pattern Extractor)
- Layer 2: Prompt 生成层 (Prompt Engineer)
- Layer 3: 评估反馈层 (Quality Assessor)
- Layer 4: 反思迭代层 (Reflection Agent)
- 融合 OPRO、Reflexion、DSPy SIMBA、PromptWizard 核心机制

**3. 测试集管理**
- 手动创建测试集
- 批量导入（txt 格式）
- 模板保存复用
- Dify 场景：连接后自动解析输入变量，用户填写变量值
- 通用 API 场景：用户自定义变量 + User Prompt
- 双任务模式：固定任务（需标准答案）/ 创意任务（只需核心诉求）

**4. 可视化（完整版）**
- React Flow 节点图
- 边动画（数据流动效果）
- 节点状态颜色（灰/蓝+脉冲/绿/红/黄）
- 流式输出（老师模型思考过程逐字符显示）
- 老师模型思考面板（右侧固定，实时同步）

**5. 用户介入**
- 任意节点可暂停
- 直接编辑中间产物
- 对话引导（告诉老师模型你的想法）
- 智能权限控制（只开放会影响后续的内容）

**6. 工作区管理**
- 多工作区支持（可并行管理多个优化任务）
- 工作区之间数据隔离
- 老师模型配置可多个、可切换

**7. 可靠性保障**
- 断点续跑（Checkpoint 机制）
- 状态持久化（SQLite）
- 失败档案 + 历史回溯
- 异常恢复（断网/断电/进程杀死）

**8. 并行测试集执行（基础并行）**
- 多条测试用例可并行运行，提升迭代效率
- 配套算法适配，确保并行 vs 串行成功率差异 < 5%
- 用户可选择串行模式（保守）或并行模式（高效）
- 高级并行策略（Racing/多候选早期淘汰）为 Phase 2 功能

**9. 元优化能力**
- 用 Prompt Faster 优化自己的老师模型 Prompt
- 多层级优化策略：
  - L0: 基础 Prompt（开发者手写，起点必须极其优秀）
  - L1: 用户可调（高级用户可修改老师模型 Prompt）
  - L2: 元优化（递归应用本系统优化老师 Prompt）
- 全版本持久化 + 成功率统计

### 8.3 Post-MVP Features

**Phase 2 (Growth)**

| 功能 | 描述 | 依赖 |
|------|------|------|
| 高级并行策略（Racing） | 多候选早期淘汰机制，进一步提升迭代效率 | 基础并行能力 |
| 更多执行引擎 | OpenAI、Anthropic、更多国内服务商 | ExecutionTarget trait 扩展 |
| 优化历史分析 | 跨任务的优化规律洞察 | 数据积累 |
| 版本对比功能 | 对比不同版本 Prompt 在任务上的差距 | 历史数据 |
| L3 数据驱动进化 | 基于成功率统计自动优化老师 Prompt | 元优化数据积累 |

**Phase 3 (Expansion)**

| 功能 | 描述 | 依赖 |
|------|------|------|
| 插件生态 | 用户可开发和共享优化组件 | 社区形成 |
| 官方优化模板库 | 经过验证的优化配置 | 最佳实践积累 |
| 商业化 | 付费高级功能、企业版 | 50+ 活跃用户 + 3 付费意向 |

### 8.4 Risk Mitigation Strategy

**技术风险**

| 风险 | 缓解措施 |
|------|----------|
| 核心算法无效 | 算法研究时间盒 6 周 + 官方基准测试集验证 + ≥70% 成功率阈值 |
| 四层架构过于复杂 | 模块化设计，每层可独立测试和迭代 |
| 并行执行质量下降 | 差异 < 5% 约束 + 用户可选串行模式 |
| 可视化实现难度 | React Flow 是成熟方案，Dify 已验证可行 |

**市场风险**

| 风险 | 缓解措施 |
|------|----------|
| 目标用户不存在 | 自用验证 + 早期用户访谈 |
| DSPy/Dify 抢先推出类似功能 | 聚焦差异化：透明可见 + 知识传递 |

**资源风险**

| 风险 | 缓解措施 |
|------|----------|
| 开发周期过长 | 3 个月时间盒，必须产出可用 MVP |
| 单人开发瓶颈 | 严格 MVP 边界，不做锦上添花功能 |

### 8.5 Scope Boundaries

**MVP 明确包含**：
- ✅ 双执行引擎（Dify + 通用 API：硅基流动、魔搭社区）
- ✅ 四层老师模型架构完整实现
- ✅ 完整可视化（动画 + 流式 + 思考面板）
- ✅ 双模式用户介入（直接编辑 + 对话引导）
- ✅ 多工作区管理
- ✅ 测试集管理（手动 + 批量导入 + 模板 + 双场景适配）
- ✅ 断点续跑 + 状态持久化
- ✅ 并行测试集执行（基础并行）
- ✅ 元优化能力（L0-L2）

**MVP 明确不包含**：
- ❌ 高级并行策略（Racing/多候选早期淘汰）（Phase 2）
- ❌ OpenAI/Anthropic 等更多执行引擎（Phase 2）
- ❌ L3 数据驱动自动进化（Phase 2，需数据积累）
- ❌ 插件生态（Phase 3）
- ❌ 多用户协作
- ❌ 云端部署（仅本地 Docker）

**高级算法特性归属表**：

| 特性 | MVP-显式功能 | MVP-内部机制 | Post-MVP |
|------|-------------|-------------|----------|
| 基础并行执行 | ✅ 用户可选串行/并行 | - | - |
| 数据划分（Train/Val/Holdout） | ✅ 用户可配置 | - | - |
| 最大迭代轮数/通过率阈值 | ✅ 用户可配置 | - | - |
| 多样性注入阈值 | ✅ 用户可配置 | - | - |
| Minibatch 采样 | - | ✅ 内部优化 | - |
| 震荡检测 | - | ✅ 内部保障 | - |
| 评估置信度门控 | - | ✅ 内部质量 | - |
| Racing 策略 | - | - | ✅ Phase 2 |
| Budget 控制 | - | - | ✅ Phase 2 |
| 多评估器组合 | - | - | ✅ Phase 2 |

> **说明**：MVP-显式功能对用户可见可配置；MVP-内部机制对用户不可见但对质量有影响；Post-MVP 为后续版本功能。

---

## Functional Requirements

**共 66 个功能需求，覆盖 10 个能力区域。**

### 能力区域 1: API 配置与连接

| FR# | 功能需求 |
|-----|----------|
| FR1 | 用户可以配置 Dify 工作流 API 凭证（地址 + API Key） |
| FR2 | 用户可以配置通用大模型 API 凭证（硅基流动、魔搭社区） |
| FR3 | 用户可以测试 API 连接是否成功 |
| FR4 | 系统可以持久化保存 API 凭证供后续使用 |
| FR5 | 用户可以配置老师模型参数（温度、top_p 等） |

### 能力区域 2: 测试集管理

| FR# | 功能需求 |
|-----|----------|
| FR6 | 用户可以手动创建测试集 |
| FR7 | 用户可以批量导入测试集（txt 格式） |
| FR8 | 用户可以将测试集配置保存为模板复用 |
| FR9 | 系统可以从 Dify API 自动解析输入变量结构 |
| FR10 | 用户可以指定哪个 Dify 变量是待优化的 system prompt |
| FR11 | 用户可以在通用 API 场景下自定义输入变量 |
| FR12 | 用户可以为固定任务提供标准答案 |
| FR13 | 用户可以为创意任务仅提供核心诉求（无标准答案） |
| FR14 | 用户可以为创意任务设置结构化约束（长度限制、必含关键词、禁止内容、格式要求） |
| FR15 | 用户可以编辑和删除已有测试集 |

### 能力区域 3: 优化任务配置

| FR# | 功能需求 |
|-----|----------|
| FR16 | 用户可以创建新的优化任务 |
| FR17 | 用户可以选择执行目标类型（Dify / 通用 API） |
| FR18 | 用户可以选择任务模式（固定任务 / 创意任务） |
| FR19 | 用户可以输入优化目标（自然语言描述） |
| FR20 | 用户可以选择填写初始 Prompt 或留空 |
| FR21 | 用户可以关联测试集到优化任务 |
| FR22 | 用户可以配置规律假设/候选 Prompt 的生成数量 |
| FR23 | 用户可以配置连续失败触发多样性注入的阈值 |
| FR23a | 用户可以配置核心算法参数（IterationConfig/DataSplitConfig/OutputConfig 等；高级参数如 Racing/Budget 在 Phase 2 后开放） |
| FR23b | 系统提供经过验证的默认配置，用户可随时一键重置为默认值 |
| FR23c | 用户可以配置最大迭代轮数、通过率阈值、数据划分策略等核心参数 |

### 能力区域 4: 自动迭代优化

| FR# | 功能需求 |
|-----|----------|
| FR24 | 系统可以执行规律抽取（Layer 1: Pattern Extractor） |
| FR25 | 系统可以生成候选 Prompt（Layer 2: Prompt Engineer） |
| FR26 | 系统可以评估 Prompt 效果（Layer 3: Quality Assessor） |
| FR27 | 系统可以执行反思迭代（Layer 4: Reflection Agent） |
| FR28 | 用户可以选择串行模式或并行模式执行测试集 |
| FR29 | 系统可以并行执行多条测试用例 |
| FR30 | 系统可以汇总并行执行结果进行综合分析 |
| FR31 | 系统可以记录失败档案并避免重蹈覆辙 |
| FR32 | 系统可以在连续失败时触发多样性注入 |
| FR33 | 系统可以自动判断优化是否成功（所有测试通过） |
| FR34 | 系统可以检测创意任务输出的多样性分数（可选功能） |

### 能力区域 5: 可视化

| FR# | 功能需求 |
|-----|----------|
| FR35 | 用户可以查看节点图形式的迭代流程 |
| FR36 | 用户可以看到节点状态颜色变化（灰/蓝/绿/红/黄） |
| FR37 | 用户可以看到边动画展示数据流动 |
| FR38 | 用户可以实时查看老师模型思考过程（流式输出） |
| FR39 | 用户可以在思考面板中看到当前环节标识 |

### 能力区域 6: 用户介入

| FR# | 功能需求 |
|-----|----------|
| FR40 | 用户可以在任意节点暂停迭代 |
| FR41 | 用户可以直接编辑当前迭代的中间产物（规律假设、Prompt） |
| FR42 | 用户可以通过对话引导老师模型（告诉它你的想法） |
| FR43 | 用户可以查看历史迭代产物（只读） |
| FR44 | 用户可以从暂停点继续迭代 |
| FR45 | 用户可以随时增加迭代轮数 |
| FR46 | 用户可以随时手动终止并选择满意的 Prompt |

### 能力区域 7: 工作区管理

| FR# | 功能需求 |
|-----|----------|
| FR47 | 用户可以创建多个工作区（优化任务） |
| FR48 | 用户可以切换查看不同工作区 |
| FR49 | 用户可以删除工作区 |
| FR50 | 系统保证工作区之间数据完全隔离 |
| FR51 | 用户可以配置多个老师模型并切换使用 |

### 能力区域 8: 可靠性与恢复

| FR# | 功能需求 |
|-----|----------|
| FR52 | 系统可以在每个迭代步骤保存 Checkpoint |
| FR53 | 系统可以在异常中断后恢复到断点状态 |
| FR54 | 用户可以从任意历史 Checkpoint 回滚 |
| FR55 | 系统可以保存完整的迭代历史记录 |

### 能力区域 9: 元优化

| FR# | 功能需求 |
|-----|----------|
| FR56 | 用户可以将老师模型 Prompt 作为优化目标 |
| FR57 | 系统可以持久化所有老师模型 Prompt 版本 |
| FR58 | 系统可以统计每个版本的成功率 |
| FR59 | 高级用户可以直接编辑老师模型 Prompt |

### 能力区域 10: 结果输出与分析

| FR# | 功能需求 |
|-----|----------|
| FR60 | 用户可以查看最终优化结果 Prompt |
| FR61 | 用户可以复制或导出 Prompt（支持 Markdown/JSON/XML） |
| FR62 | 用户可以对比任意两个 Prompt 版本在同一测试集上的效果差异 |
| FR63 | 用户可以查看诊断报告了解"为什么之前不行" |

---

## Non-Functional Requirements

**共 28 个非功能需求，覆盖 8 个质量属性类别。**

### 性能 (Performance)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR1 | 系统自身延迟 | < 100ms | 不含大模型 API 调用时间 |
| NFR2 | 流式输出首字节延迟 | < 500ms | 老师模型思考过程开始显示的时间 |
| NFR3 | 节点图渲染性能 | 60fps | 动画流畅，无卡顿 |
| NFR4 | 并行测试集执行效率 | 接近线性加速 | N 条并行约为串行时间的 1/N |

### 可靠性 (Reliability)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR5 | 断点恢复率 | 100% | 覆盖 kill/断网/断电/跨版本场景 |
| NFR6 | 数据持久化保障 | WAL 模式 | SQLite 使用 WAL + FULL synchronous |
| NFR7 | Checkpoint 完整性 | 100% | 每步写入成功后才标记完成 |
| NFR8 | API 调用重试 | 自动重试 3 次 | 网络波动时自动重试 |

### 安全性 (Security)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR9 | API Key 存储 | 加密存储 | 本地数据库中加密保存 |
| NFR10 | 凭证传输 | 仅本地 | API Key 不离开用户设备 |
| NFR11 | 日志脱敏 | 自动脱敏 | 日志中不记录完整 API Key |
| NFR11a | 本地用户认证 | 支持本地登录 | 为未来扩展（云同步、多设备）打基础 |
| NFR11b | 用户数据隔离 | 按用户隔离 | 不同用户的工作区和数据完全隔离 |

### 可扩展性 (Extensibility)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR12 | 新增执行引擎 | < 4 小时 | 实现 ExecutionTarget trait |
| NFR13 | 新增评估器 | < 2 小时 | 实现 Evaluator trait |
| NFR14 | 新增老师模型 | < 2 小时 | 实现 TeacherModel trait |
| NFR15 | 核心算法替换 | 仅影响算法模块 | 模块化隔离，不影响其他功能 |

### 可用性 (Usability)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR16 | 安装时间 | < 5 分钟 | Docker Compose 一键启动 |
| NFR17 | 首次完成优化 | < 30 分钟 | 新用户上手验证 |
| NFR18 | 界面语言 | 中文为主，英文为辅 | 支持切换 |

### 测试覆盖 (Quality Assurance)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR19 | 核心流程端到端测试 | ≥ 80% 覆盖率 | 关键路径必须覆盖 |
| NFR20 | 模块回归测试 | 100% 通过 | 修改后必须通过 |
| NFR21 | 优化成功率基准 | ≥ 90% | 官方基准测试集验证 |
| NFR22 | 并行 vs 串行差异 | < 5% | 并行不牺牲质量 |

### 错误处理 (Error Handling)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR23 | API 调用超时阈值 | ≤ 60 秒 | 超时后自动重试或友好提示 |
| NFR24 | 错误信息可读性 | 100% 覆盖 | 所有错误提供用户可理解的解决建议 |

### 资源与离线 (Resource & Offline)

| NFR# | 需求 | 目标值 | 说明 |
|------|------|--------|------|
| NFR25 | 内存占用限制 | 空闲 ≤ 500MB，运行 ≤ 2GB | 本地应用资源友好 |
| NFR26 | 离线功能可用性 | 100% 本地功能可用 | 无网络时历史查看、导出等仍可用 |
